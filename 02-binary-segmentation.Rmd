# Binary segmentation

In this chapter we will discuss the binary segmentation algorithm,
which is perhaps the most popular classical heuristic for changepoint
detection. For a sequence of N data, binary segmentation defines a
regularization path of models from 1 to N segments. Starting with the
simplest model with just one segment (no changepoints), the binary
segmentation algorithm recursively performs the following computations:
- for each segment, compute the loss for all new changepoints that are
  possible on this segment. Store the best/min loss for this segment
  along with the optimal split point.
- Split the current segment which has the best/min loss.

Binary segmentation therefore defines a hierarchical sequence of
splits: the model with K changepoints contains all of the changepoints
in the previous model with K-1 changepoints. Therefore binary
segmentation may compute a sub-optimal set of changepoints (for
$S\in\{3,\dots,N-1\}$ segments), which is why it is a
heuristic. Nevertheless, binary segmentation is an interesting and
useful algorithm, because it is very fast, and can often compute a
good approximation of the optimal changepoints.

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

## Segmentation / changepoint detection framework

- Let $x_1, \dots, x_n \in\mathbb R$ be a data sequence
  over space or time (logratio column in DNA copy number data below).
- Where are the abrupt changes in the data sequence?
- There are a certain number of clusters/segments
  $K\in\{1,\dots, n\}$.
- Each segment $k\in\{1,\dots,K\}$ has its own mean
  parameter $\mu_k\in\mathbb R$.
- There is some constant variance parameter $\sigma^2>0$ which is
  common to all segments.
- For each data point $i$ on segment
  $k\in\{1,\dots,K\}$ we have $x_i \sim N(\mu_k, \sigma^2)$ -- normal
  distribution.
- This normal distribution assumption means that we want to find
  segments/changepoints with mean $m$ that minimize the square loss,
  $(x-m)^2$.
- Other distributional assumptions / loss functions are possible.

```{r results=TRUE}
data(neuroblastoma, package="neuroblastoma")
library(data.table)
library(ggplot2)
nb.profiles <- data.table(neuroblastoma[["profiles"]])
one.pid.chr <- nb.profiles[profile.id==79 & chromosome==1]
one.pid.chr <- nb.profiles[profile.id==22 & chromosome==12]
one.pid.chr <- nb.profiles[profile.id==2 & chromosome==2]
one.pid.chr
one.pid.chr[, data.i := .I]
gg <- ggplot()+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=one.pid.chr)
gg
```

## Computing the binary segmentation model using R packages

In this section we explain how to use various R packages which
implement binary segmentation.

- binsegRcpp: only implements normal change in mean (with constant variance).
- changepoint: implements many more statistical models.
- BinSeg: implements most of the same models as in changepoint.

The `binsegRcpp::binseg_normal` function implements the normal change
in mean model (with constant variance). It can be used via

```{r}
max.segments <- 5
bs.models <- binsegRcpp::binseg_normal(
  one.pid.chr[["logratio"]],
  max.segments)
model.color <- "blue"
k.segs <- coef(bs.models)
gg+
  geom_vline(aes(
    xintercept=start-0.5),
    color=model.color,
    data=k.segs[start>1])+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    color=model.color,
    data=k.segs)+
  facet_grid(segments ~ ., labeller=label_both)
```

The `changepoint::cpt.mean` function implements the same algorithm via
the code below (which gives a different result from binsegRcpp, why?)

```{r}
cp.models.manual <- changepoint::cpt.mean(
  one.pid.chr[["logratio"]],
  method="BinSeg",
  penalty="Manual",
  pen.value=0,
  Q=max.segments-1)
(cpt.mat <- changepoint::cpts.full(cp.models.manual))
seg.dt <- data.table(
  segments=as.integer(row(cpt.mat)+1),
  end=as.integer(cpt.mat)
)[!is.na(end)]
gg+
  geom_vline(aes(
    xintercept=end+0.5),
    color=model.color,
    data=seg.dt)+
  facet_grid(segments ~ ., labeller=label_both)
```

Another way to run it is via the code below (which gives different
results from the two methods above, why?)

```{r}
cp.models.default <- changepoint::cpt.mean(
  one.pid.chr[["logratio"]],
  method="BinSeg",
  Q=max.segments-1)
(cpt.mat <- changepoint::cpts.full(cp.models.default))
seg.dt <- data.table(
  segments=as.integer(row(cpt.mat)+1),
  end=as.integer(cpt.mat)
)[!is.na(end)]
gg+
  geom_vline(aes(
    xintercept=end+0.5),
    color=model.color,
    data=seg.dt)+
  facet_grid(segments ~ ., labeller=label_both)
```

Finally we can also use the BinSeg package,

```{r}
BS.models <- BinSeg::BinSegModel(
  one.pid.chr[["logratio"]],
  algorithm="BS",
  distribution="mean_norm",
  numCpts=max.segments-1)
k.segs <- BinSeg::coef(BS.models)
gg+
  geom_vline(aes(
    xintercept=start-0.5),
    color=model.color,
    data=k.segs[start>1])+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    color=model.color,
    data=k.segs)+
  facet_grid(segments ~ ., labeller=label_both)
```

Why the difference between BinSeg and binsegRcpp? It seems that the
loss is the same until the fifth segment model, for which binsegRcpp
has smaller loss. Maybe BinSeg is not allowing segments witha single
data point?

```{r}
rbind(
  binsegRcpp=bs.models$loss*2,
  BinSeg=BS.models@models_summary$cost)
```

## Implementing and visualizing binary segmentation in R

In this section we provide a pure R implementation of binary
segmentation, for the purposes of explaining how it works.

The main idea of the algorithm is to keep a list of segments that
could be split, and then choose the split with minimal loss. To keep
track of the segments we can use a data table with one row for each
segment that could be split. Initially this table has only one row
corresponding to the full data set,

```{r}
(first.seg.dt <- data.table(
  full_seg_start=1L,
  full_seg_end=nrow(one.pid.chr)))
```

The column names in the table above refer to the full segment on which
we want to search for the split point. The next step is to compute the
loss of each possible split,

```{r}
cum.data.vec <- c(0, cumsum(one.pid.chr[["logratio"]]))
possible_splits <- function(seg.dt){
  possible.dt <- seg.dt[full_seg_start<full_seg_end, {
    before_seg_end <- seq(full_seg_start, full_seg_end-1)
    data.table(
      before_seg_start=full_seg_start,
      before_seg_end,
      after_seg_start=before_seg_end+1L,
      after_seg_end=full_seg_end
    )
  }, by=.(full_seg_start, full_seg_end)]
  name <- function(suffix)paste0(seg_name, "_seg_", suffix)
  value <- function(suffix)possible.dt[[name(suffix)]]
  for(seg_name in c("before", "after", "full")){
    end <- value("end")
    start <- value("start")
    N.data <- end-start+1
    sum.data <- cum.data.vec[end+1]-cum.data.vec[start]
    set(
      possible.dt,
      j=name("loss"),
      value=-sum.data^2/N.data)
    set(
      possible.dt,
      j=name("mean"),
      value=sum.data/N.data)
  }
  possible.dt[
  , split_loss := before_seg_loss + after_seg_loss][
  , loss_diff := split_loss-full_seg_loss][]
}
first.possible <- possible_splits(first.seg.dt)
(first.best <- first.possible[which.min(loss_diff)])
get_segs <- function(best){
  nc::capture_melt_multiple(
    best,
    seg="before|after",
    "_seg_",
    column="start|end|mean"
  )[order(start)][, startChange := c(FALSE, TRUE)]
}
first.best.tall <- get_segs(first.best)
ggplot()+
  facet_grid(panel ~ ., scales="free")+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(panel="data", one.pid.chr))+
  geom_point(aes(
    before_seg_end+0.5, loss_diff),
    shape=1,
    data=data.table(panel="loss", first.possible))+
  geom_vline(aes(
    xintercept=before_seg_end+0.5),
    data=first.best,
    color=model.color)+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    color=model.color,
    size=2,
    alpha=0.5,
    data=data.table(panel="data", first.best.tall))
```

The second split point is computed in almost the same way,

```{r}
(second.segs <- first.best.tall[, .(
  full_seg_start=start, full_seg_end=end)])
second.possible <- possible_splits(second.segs)
second.splits.dt <- second.possible[, {
  .SD[which.min(loss_diff)]
}, by=.(full_seg_start, full_seg_end)]
second.best.i <- second.splits.dt[, which.min(loss_diff)]
second.best <- second.splits.dt[second.best.i]
second.best.tall <- get_segs(second.best)
second.segs.dt <- rbind(
  data.table(
    computed="previously",
    first.best.tall[
      !second.best,
      on=c(start="full_seg_start", end="full_seg_end")]),
  data.table(
    computed="this step", second.best.tall))
get_vlines <- function(segs){
  segs[start>1][startChange==FALSE, computed := "previously"]
}
second.vlines <- get_vlines(second.segs.dt)
ggplot()+
  facet_grid(panel ~ ., scales="free")+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(panel="data", one.pid.chr))+
  geom_point(aes(
    before_seg_end+0.5, loss_diff,
    color=computed),
    shape=1,
    data=data.table(
      panel="loss", computed="this step", second.possible))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=data.table(
      panel="data",
      second.vlines[computed=="previously"]))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=second.vlines[computed=="this step"])+
  geom_segment(aes(
    start-0.5, mean,
    color=computed,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    data=data.table(panel="data", second.segs.dt))
```

Then for the third step we need to minimize over previously computed
split, and new splits.

```{r}
(third.segs <- second.best.tall[, .(
  full_seg_start=start, full_seg_end=end)])
third.possible <- possible_splits(third.segs)
third.possible.show <- rbind(
  data.table(
    computed="previously",
    second.splits.dt[-second.best.i]),
  data.table(
    computed="this step", third.possible))
(third.splits.dt <- rbind(
  second.splits.dt[-second.best.i],
  third.possible[, {
    .SD[which.min(loss_diff)]
  }, by=.(full_seg_start, full_seg_end)]))
```

The code above combines the previously unchosen split from the second
step, with the two new splits from the third step. 

```{r}
third.best.i <- third.splits.dt[, which.min(loss_diff)]
third.best <- third.splits.dt[third.best.i]
third.best.tall <- get_segs(third.best)
third.segs.dt <- rbind(
  second.segs.dt[
    !third.best, on=c(start="full_seg_start", end="full_seg_end")
  ][, computed := "previously"],
  data.table(computed="this step", third.best.tall))
third.vlines <- get_vlines(third.segs.dt)
ggplot()+
  facet_grid(panel ~ ., scales="free")+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(panel="data", one.pid.chr))+
  geom_point(aes(
    before_seg_end+0.5, loss_diff, color=computed),
    shape=1,
    data=data.table(panel="loss", third.possible.show))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=data.table(
      panel="data",
      third.vlines[computed=="previously"]))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=third.vlines[computed=="this step"])+
  geom_segment(aes(
    start-0.5, mean,
    color=computed,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    data=data.table(panel="data", third.segs.dt))
```

The fourth step is similar,

```{r}
(fourth.segs <- third.best.tall[, .(
  full_seg_start=start, full_seg_end=end)])
fourth.possible <- possible_splits(fourth.segs)
fourth.possible.show <- rbind(
  data.table(
    computed="previously",
    third.splits.dt[-third.best.i]),
  data.table(
    computed="this step", fourth.possible))
(fourth.splits.dt <- rbind(
  third.splits.dt[-third.best.i],
  fourth.possible[, {
    .SD[which.min(loss_diff)]
  }, by=.(full_seg_start, full_seg_end)]))
fourth.best.i <- fourth.splits.dt[, which.min(loss_diff)]
fourth.best <- fourth.splits.dt[fourth.best.i]
fourth.best.tall <- get_segs(fourth.best)
fourth.segs.dt <- rbind(
  third.segs.dt[
    !fourth.best, on=c(start="full_seg_start", end="full_seg_end")
  ][, computed := "previously"],
  data.table(computed="this step", fourth.best.tall))
fourth.vlines <- get_vlines(fourth.segs.dt)
ggplot()+
  facet_grid(panel ~ ., scales="free")+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(panel="data", one.pid.chr))+
  geom_point(aes(
    before_seg_end+0.5, loss_diff, color=computed),
    shape=1,
    data=data.table(panel="loss", fourth.possible.show))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=data.table(
      panel="data",
      fourth.vlines[computed=="previously"]))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=fourth.vlines[computed=="this step"])+
  geom_segment(aes(
    start-0.5, mean,
    color=computed,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    data=data.table(panel="data", fourth.segs.dt))
```

The fifth step is similar,

```{r}
(fifth.segs <- fourth.best.tall[, .(
  full_seg_start=start, full_seg_end=end)])
(fifth.possible <- possible_splits(fifth.segs))
fifth.possible.show <- rbind(
  data.table(
    computed="previously",
    fourth.splits.dt[-fourth.best.i]),
  data.table(
    computed="this step", fifth.possible))
(fifth.splits.dt <- rbind(
  fourth.splits.dt[-fourth.best.i],
  fifth.possible[, {
    .SD[which.min(loss_diff)]
  }, by=.(full_seg_start, full_seg_end)]))
fifth.best.i <- fifth.splits.dt[, which.min(loss_diff)]
fifth.best <- fifth.splits.dt[fifth.best.i]
fifth.best.tall <- get_segs(fifth.best)
fifth.segs.dt <- rbind(
  fourth.segs.dt[
    !fifth.best, on=c(start="full_seg_start", end="full_seg_end")
  ][, computed := "previously"],
  data.table(computed="this step", fifth.best.tall))
fifth.vlines <- get_vlines(fifth.segs.dt)
ggplot()+
  facet_grid(panel ~ ., scales="free")+
  scale_x_continuous(
    limits=c(0, nrow(one.pid.chr)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=data.table(panel="data", one.pid.chr))+
  geom_point(aes(
    before_seg_end+0.5, loss_diff, color=computed),
    shape=1,
    data=data.table(panel="loss", fifth.possible.show))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=data.table(
      panel="data",
      fifth.vlines[computed=="previously"]))+
  geom_vline(aes(
    xintercept=start-0.5,
    color=computed),
    data=fifth.vlines[computed=="this step"])+
  geom_segment(aes(
    start-0.5, mean,
    color=computed,
    xend=end+0.5, yend=mean),
    size=2,
    alpha=0.5,
    data=data.table(panel="data", fifth.segs.dt))
```

## Other distributional assumptions

Above we have mainly discussed the normal model with change in mean
(and constant variance). However the binary segmentation algorithm is
much more widely applicable to other statistical models and data. For
example, consider the wave data below TODO.

```{r}
data(wave.c44137, package="changepoint")
wave.dt <- data.table(height=wave.c44137)
## The data are taken at hourly intervals from January 2005 until
## September 2012.
ggplot()+
  geom_line(aes(
    seq_along(height), height),
    data=wave.dt)
```

## Complexity analysis

- Assume $n$ data and $K$ segments.
- Computing best loss decrease and split point for a segment with $t$
  data takes $O(t)$ time.
- Keep a list of segments which could be split, sorted by loss
  decrease values.
- Best case is when segments get cut in half each time, $O(n \log K)$
  time. (minimize number of possible splits for which we have to recompute loss)
- Worst case is when splits are very unequal (1, $t-1$), $O(n K)$
  time. (maximize number of possible splits for which we have to
  recompute loss)

## Possible exam questions

- Explain in detail one similarity and one difference between binary
  segmentation and k-means. (gaussian mixture models, hierarchical
  clustering)
- For a sequence of $n=10$ data, we need to compute the loss for each
  of the 9 possible splits in the first iteration of binary
  segmentation. What is the number of splits for which we must compute
  the loss in the second step? (best and worst case)
