# Binary segmentation

Here is a review of existing methods.

```{r opts, echo=FALSE}
knitr::opts_chunk$set(
  echo=FALSE, results=FALSE,
  fig.width=10,
  fig.height=6)
```

## Segmentation / changepoint detection framework

- Let $x_1, \dots, x_n \in\mathbb R$ be a data sequence
  over space or time (logratio column in DNA copy number data below).
- Where are the abrupt changes in the data sequence?

```{r results=TRUE}
data(neuroblastoma, package="neuroblastoma")
library(data.table)
library(ggplot2)
nb.dt <- data.table(neuroblastoma[["profiles"]])
one.dt <- nb.dt[profile.id==79 & chromosome==1]
one.dt <- nb.dt[profile.id==22 & chromosome==12]
one.dt
```

## Assume normal distribution with change in mean, constant variance

- There are a certain number of clusters/segments
  $K\in\{1,\dots, n\}$.
- Each segment $k\in\{1,\dots,K\}$ has its own mean
  parameter $\mu_k\in\mathbb R$.
- There is some constant variance parameter $\sigma^2>0$ which is
  common to all segments.
- For each data point $i$ on segment
  $k\in\{1,\dots,K\}$ we have $x_i \sim N(\mu_k, \sigma^2)$ -- normal
  distribution.
- This normal distribution assumption means that we want to find
  segments/changepoints with mean $m$ that minimize the square loss,
  $(x-m)^2$.
- Other distributional assumptions / loss functions are possible.

## Computing the binary segmentation model

```{r}
one.dt[, data.i := .I]
gg <- ggplot()+
  scale_x_continuous(
    limits=c(0, nrow(one.dt)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=one.dt)
```

Binary segmentation defines a regularization path of models from 1 to
N segments. The simplest model has one segment, which has the mean of
all the data points for the case of normal model (with change in mean
and no change in variance).

```{r}
bs.models <- binsegRcpp::binseg_normal(one.dt[["logratio"]])
model.color <- "blue"
plotK <- function(k){
  k.segs <- coef(bs.models, as.integer(k))
  gg+
    geom_vline(aes(
      xintercept=start-0.5),
      color=model.color,
      data=k.segs[-1])+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      size=2,
      alpha=0.5,
      color=model.color,
      data=k.segs)
}
plotK(1)
```

The second model in the regularization path has two,

```{r}
plotK(2)
```

## Find two changepoints (three segments)

```{r}
plotK(3)
```

## Find four segments

```{r}
plotK(4)
```

## Find five segments

```{r}
plotK(5)
```

## Find six segments

```{r}
plotK(6)
```

## Find seven segments

```{r}
plotK(7)
```

## Find 50 segments

```{r}
plotK(50)
```

## Find 100 segments

```{r}
plotK(100)
```

## Largest model: `r nrow(one.dt)` segments (changes everywhere)

```{r}
plotK(nrow(one.dt))
```

## Error/loss function visualization

- Let $m^{(k)}\in\mathbb R^n$ be the mean vector with $k$ segments.
- Error for $k$ segments is defined as sum of squared difference
  between data $x$ and mean $m^{(k)}$ vectors, $E_k = \sum_{i=1}^n
  (x_i-m^{(k)}_i)^2$
- As in previous clustering models, kink in the error curve can be
  used as a simple model selection criterion.

```{r fig.height=5}
ggplot()+
  geom_line(aes(
    segments, loss),
    data=bs.models)
```

## Error/loss function zoom

```{r}
show.max <- 10
ggplot()+
  geom_point(aes(
    segments, loss),
    data=bs.models[segments<show.max])+
  scale_x_continuous(breaks=1:show.max)
```

## Learning algorithm

- Start with one segment, then repeat:
- Compute loss of each possible split.
- Choose split which results in largest loss decrease.
- If $s = \sum_{i=1}^n x_i$ is the sum over $n$ data points, then the
  mean is $s/n$ and the square loss (from 1 to $n$) is
$$L_{1,n} = \sum_{i=1}^n (x_i - s/n)^2 = \sum_{i=1}^n [x_i^2] - 2(s/n)s + n(s/n)^2 $$
- Use cumulative sum to compute square loss from 1 to $t$, $L_{1,t}$, and from $t+1$ to $n$, $L_{t+1,n}$, and minimize over all changepoints $t$,
$$\min_{t\in\{1,\dots,n-1\}} L_{1,t} + L_{t+1,n}$$

## First step of binary segmentation

```{r}
x.vec <- one.dt[["logratio"]]
n.data <- length(x.vec)
hjust.vec <- c("before"=1, "after"=0)
before.after <- function(x.or.sq){
  cbind(
    before=cumsum(x.or.sq[-length(x.or.sq)]),
    after=rev(cumsum(rev(x.or.sq[-1]))))
}
end.vec <- seq(1, n.data-1)
n.mat <- cbind(end.vec, rev(end.vec))
const.mat <- before.after(x.vec^2)
s.mat <- before.after(x.vec)
l.mat <- const.mat-s.mat^2/n.mat
err.dt <- data.table(
  loss=l.mat,
  loss.total=rowSums(l.mat),
  mean=s.mat/n.mat,
  end=end.vec)
first.min <- err.dt[which.min(loss.total)]

plotChange <- function(show.i){
  show.change <- err.dt[show.i]
  show.long <- melt(
    show.change,
    measure=c("loss.before", "loss.after"))
  show.segs <- data.table(
    start=c(1, show.i+1),
    end=c(show.i, n.data),
    mean=s.mat[show.i,]/n.mat[show.i,])
  ggplot()+
    geom_point(aes(
      data.i, logratio),
      data=data.table(y="logratio", one.dt))+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      data=show.change)+
    geom_text(aes(
      end+0.5, -Inf, label=sprintf(" total loss=%.4f", loss.total)),
      color=model.color,
      hjust=0,
      vjust=-0.1,
      data=data.table(y="loss", show.change))+
    geom_text(aes(
      end+0.5, -Inf,
      label=sprintf(" %s=%.4f ", variable, value),
      hjust=hjust.vec[sub("loss.","",variable)]),
      color=model.color,
      data=data.table(y="logratio", show.long),
      vjust=-0.1)+
    geom_point(aes(
      end+0.5, loss.total),
      shape=21,
      data=data.table(y="loss", err.dt))+
    facet_grid(y ~ ., scales="free")+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      size=2,
      alpha=0.5,
      color=model.color,
      data=data.table(y="logratio", show.segs))
}
plotChange(31)
```

## First step of binary segmentation

```{r}
plotChange(41)
```

## First step of binary segmentation

```{r}
plotChange(111)
```

## First step of binary segmentation

```{r}
plotChange(157)
```

## Second step of binary segmentation

```{r}
(seg.info <- first.min[, data.table(
  start=c(1, end+1),
  end=c(end, nrow(one.dt)),
  seg=factor(c("before", "after"), c("before", "after")),
  offset=c(0, end))])
seg.ord <- list(first=identity, other=rev)
for(name in names(seg.ord)){
  fun <- seg.ord[[name]]
  for(data.type in c("mean","loss")){
    col.vec <- paste0(data.type,".",c("before","after"))
    orig.ord <- unlist(first.min[, ..col.vec])
    set(seg.info, j=paste0(name, ".", data.type), value=fun(orig.ord))
  }
}
seg.info

one.dt[, i := data.i]
setkey(one.dt, data.i, i)
setkey(seg.info, start, end)
two.dt <- foverlaps(one.dt, seg.info)
##two.dt <- data.table(one.dt)[seg.info, on=.(i <= end, i >= start)]
two.err <- two.dt[, {
  rel.end <- seq(1, .N-1)
  n.mat <- cbind(rel.end, rev(rel.end))
  const.mat <- before.after(logratio^2)
  s.mat <- before.after(logratio)
  l.mat <- const.mat-s.mat^2/n.mat
  data.table(
    mean=s.mat/n.mat,
    loss.this=rowSums(l.mat),
    loss.decrease=first.loss-rowSums(l.mat),
    loss=other.loss+rowSums(l.mat),
    new.end=rel.end+offset)
}, by=names(seg.info)]

plot2 <- function(show.i){
  (show.err <- two.err[show.i])
  show.other <- seg.info[seg != show.err$seg]
  show.change <- show.err[, .(seg, start, end=new.end, mean=mean.before)]
  show.segs <- rbind(
    show.other[, .(seg, start, end, mean=first.mean)],
    show.change,
    show.err[, .(seg, start=new.end+1, end, mean=mean.after)])
  ggplot()+
    geom_point(aes(
      data.i, logratio),
      data=data.table(y="logratio", two.dt))+
    geom_point(aes(
      new.end+0.5, loss),
      shape=21,
      data=data.table(y="loss", two.err))+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      size=2,
      alpha=0.5,
      color=model.color,
      data=data.table(y="logratio", show.segs))+
    geom_vline(aes(
      xintercept=end+0.5),
      color=model.color,
      data=show.change)+
    facet_grid(y ~ ., scales="free", space="free")+
    scale_x_continuous(
      breaks=seq(0, 1000, by=20))+
    coord_cartesian(expand=TRUE)
}
plot2(15)

```

## Second step of binary segmentation

```{r}
plot2(30)
```

## Second step of binary segmentation

```{r}
plot2(100)
```

## Second step of binary segmentation

```{r}
plot2(112)
```

## Second step of binary segmentation

```{r}
plot2(156)
```

## Second step of binary segmentation

```{r}
plot2(200)
```

## Loss computation

- Minimization can be performed by choosing the split with loss (black
  point) which maximizes the decrease in loss with respect to previous
  model (red, with no split).

```{r}
first.min.long <- nc::capture_melt_single(
  first.min, "loss.",
  seg="before|after", function(x)factor(x, c("before", "after")))
ggplot()+
  geom_hline(aes(
    yintercept=value),
    color="red",
    data=data.table(
      y="loss on this segment", first.min.long))+
  ### geom_point(aes(
  ###   end+0.5, loss.decrease),
  ###   shape=21,
  ###   data=data.table(y="loss decrease", two.err))+
  geom_point(aes(
    new.end+0.5, loss),
    shape=21,
    data=data.table(y="total loss", two.err))+
  geom_point(aes(
    new.end+0.5, loss.this),
    shape=21,
    data=data.table(y="loss on this segment", two.err))+
  facet_grid(y ~ seg, scales="free", space="free_x")+
  scale_x_continuous(
    "data.i",
    breaks=seq(0, 1000, by=20))

```

## Comparison with previous algorithms from clustering

- Binary segmentation has segment/cluster-specific mean parameter, as
  in K-means and Gaussian mixture models. These algorithms attempt
  optimization of an error function which measures how well the means
  fit the data (but are not guaranteed to compute the globally
  optimal/best model).
- Binary segmentation is deterministic (different from K-means/GMM
  which requires random initialization). It performs a sequence of
  greedy minimizations (as in hierarchical clustering).
- Binary segmentation defines a sequence of split operations (from 1
  segment to N segments), whereas agglomerative hierarchical
  clustering defines a sequence of join operations (from N clusters to
  1 cluster). Data with common segment mean must be adjacent in
  time/space; hierarchical clustering joins may happen between any
  pair of data points (no space/time dimension).

## Complexity analysis

- Assume $n$ data and $K$ segments.
- Computing best loss decrease and split point for a segment with $t$
  data takes $O(t)$ time.
- Keep a list of segments which could be split, sorted by loss
  decrease values.
- Best case is when segments get cut in half each time, $O(n \log K)$
  time. (minimize number of possible splits for which we have to recompute loss)
- Worst case is when splits are very unequal (1, $t-1$), $O(n K)$
  time. (maximize number of possible splits for which we have to
  recompute loss)

## Possible exam questions

- Explain in detail one similarity and one difference between binary
  segmentation and k-means. (gaussian mixture models, hierarchical
  clustering)
- For a sequence of $n=10$ data, we need to compute the loss for each
  of the 9 possible splits in the first iteration of binary
  segmentation. What is the number of splits for which we must compute
  the loss in the second step? (best and worst case)
